{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelToLoad = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Memory commands\n",
    "\n",
    "print(torch.cuda.memory_allocated()/1024**3)\n",
    "print(torch.cuda.memory_cached()/1024**3)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# x = torch.randn(1024**3, device='cuda')\n",
    "# device='cuda:0'\n",
    "# x = x.cuda()\n",
    "# del y\n",
    "# print(x.dtype)\n",
    "# print('OnGPUMemory:', x.element_size()*x.nelement()/1024**3, 'GB')\n",
    "\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.max_memory_allocated()/1024**3\n",
    "# torch.cuda.memory_reserved()\n",
    "# torch.cuda.max_memory_reserved()/1024**3\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_stats()\n",
    "# torch.cuda.memory_snapshot()\n",
    "\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.memory_usage()\n",
    "# torch.cuda.list_gpu_processes() ####################################################\n",
    "# print(torch.cuda.memory_summary()) #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, class definitions\n",
    "# print(\"Importing ...\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms # Using TorchIO may help in 3D augmentation *\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "# Define your model architecture here\n",
    "# print(\"Defining Classes ...\")\n",
    "\n",
    "# Class UNet3D\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels , out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module): #\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffZ = x2.size()[2] - x1.size()[2] # NCXYZ\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1 = nn.functional.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2,\n",
    "                                    diffZ // 2, diffZ - diffZ // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module): ### Add dropout!\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "# Define a custom transform class for applying the same random crop\n",
    "class RandomCrop3D: ###\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "\n",
    "        # Get the input size\n",
    "        input_size = inputs.shape[2:] ###\n",
    "\n",
    "        # Calculate the starting index for the crop\n",
    "        start_indexes = [random.randint(0, input_size[i] - self.output_size[i]) for i in range(3)]\n",
    "\n",
    "        # Perform the crop on both inputs and targets\n",
    "        inputs  = inputs [:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        targets = targets[:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "# Define the output size for random cropping\n",
    "output_size = (128, 128, 128)\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    RandomCrop3D(output_size),              # Custom random crop\n",
    "    # transforms.RandomVerticalFlip(),        # Random vertical flipping\n",
    "    # transforms.RandomHorizontalFlip()        # Random horizontal flipping\n",
    "])\n",
    "\n",
    "# Define your dataset class for loading CT images and masks\n",
    "\n",
    "class CTImageDataset(torch.utils.data.Dataset): ### Yields with 1 channel dim [No batch dim yet]\n",
    "    def __init__(self, image_paths, mask_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = nib.load(self.image_paths[index]).get_fdata()\n",
    "        mask  = nib.load(self.mask_paths [index]).get_fdata()\n",
    "        image = torch.from_numpy(image).unsqueeze(0).float() ### 1-Channel?!\n",
    "        mask  = torch.from_numpy(mask ).unsqueeze(0).long() ### Changed!\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function definition\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device): ###\n",
    "    model.train() ###\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        # print(f\"Batch {batch_idx+1} Started\")\n",
    "\n",
    "        images = images.to(device)\n",
    "        masks  = masks .to(device)\n",
    "\n",
    "        # Apply transforms to the inputs and targets\n",
    "        images, masks = transform((images, masks))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # print(\"Passing through Model ...\")\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        # print(\"CrossEnthropy() ...\")\n",
    "        loss = criterion(outputs, torch.squeeze(masks, dim=1)) #####################################\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # print(\"Backward ...\")\n",
    "        loss.backward()\n",
    "        # print(\"Step ...\")\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Should I detach() the output?!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader), images[0,0].cpu(), masks[0,0].cpu(), outputs[0].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters: device, epochs, lr, ..\n",
    "# print(\"Setting Parameters & Instanciating ...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ####1\n",
    "epochs = 4\n",
    "batch_size = 1 #4 ###\n",
    "learning_rate = 0.0003 #0.0001 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new model instance & load M-Model-N\n",
    "\n",
    "model = UNet3D(in_channels=1, out_channels=3)\n",
    "model = model.to(device)\n",
    "# GMem+=0.4GB > 3.1GB [>1.5GB]\n",
    "model.load_state_dict(torch.load(\"./Models/M-Model-\"+f'{ModelToLoad:02.0f}'+\".pth\")) ########################## m1\n",
    "# GMem+=0.6GB > 3.7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count the number of trainable parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_parameters}\")\n",
    "# Number of trainable parameters: 40158723\n",
    "\n",
    "# Model size\n",
    "print(\"Model size assuming float32bit weights:\", 40158723*32/8/1024**2, \"MB\")\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters table\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Mod name\", \"Parameters Listed\"])\n",
    "    t_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        t_params+=param\n",
    "    print(table)\n",
    "    print(f\"Sum of trained paramters: {t_params}\")\n",
    "    return t_params\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image paths, criterion/optimizer\n",
    "\n",
    "# Create your dataset and data loader instances\n",
    "import os\n",
    "IDS = [36, 43, 54]\n",
    "image_paths = [os.path.join('Data','SPIROMCS-Case'+str(ID)+'-Vx3.nii.gz') for ID in IDS]\n",
    "mask_paths  = [os.path.join('Data','SPIROMCS-Case'+str(ID)+'-012Labelmap.nii.gz') for ID in IDS]\n",
    "\n",
    "train_dataset = CTImageDataset(image_paths[0:2], mask_paths[0:2]) ### Cases 36,43\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) ### Mask: B=1?C=1?XYZ? #shuffle=True\n",
    "\n",
    "valid_dataset = CTImageDataset( [ image_paths[2] ] , [ mask_paths[2] ] ) ### Cases 54\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) ### Adds Batch dim  ## Mask: B=1?C=1?XYZ? #shuffle=True\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "\n",
    "weight = torch.tensor([1,10,10], dtype=torch.float, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) ####2 ignore_index (int, optional) *** #### Using max of all N losses?!!!!\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS = [36, 43, 54]\n",
    "\n",
    "image = nib.load(image_paths[0]).get_fdata()\n",
    "mask  = nib.load(mask_paths [0]).get_fdata()\n",
    "image = torch.from_numpy(image).unsqueeze(0).float() ### 1-Channel?!\n",
    "mask  = torch.from_numpy(mask ).unsqueeze(0).long() ### Changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ToDo: Get label distribution for calss weights\n",
    "# Here!\n",
    "# np.array([1,10,10])/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desc; Read file and create train_losses, unqlabels, unqlabelindices\n",
    "\n",
    "desc = 'Using-' + f'{ModelToLoad:02.0f}' + '|LR=' + str(learning_rate) + '|x' + str(epochs)\n",
    "PlotOnlyTheLastNLabels = 6\n",
    "\n",
    "# LastIteration, train_loss, labels\n",
    "\n",
    "train_losses, valid_losses, labels  = [],[],[] #>>\n",
    "\n",
    "with open('D-LossValues.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\", \")\n",
    "        train_losses.append(float(parts[1]))\n",
    "        valid_losses.append(float(parts[2]))\n",
    "        labels.append(parts[3])\n",
    "        LastIteration = int(parts[0])\n",
    "\n",
    "# unqlabels, unqlabelindices, howmanyinfile\n",
    "\n",
    "unqlabels = [] #>>\n",
    "unqlabelindices = [] #>>\n",
    "currentlabel = []\n",
    "\n",
    "for indx,lab in enumerate(labels):\n",
    "    if lab != currentlabel:\n",
    "        unqlabelindices.append(indx)\n",
    "        unqlabels.append(lab)\n",
    "        currentlabel = lab\n",
    "\n",
    "howmanyinfile = len(train_losses) # Used only once to append to unqlabelindices\n",
    "\n",
    "# Trimming unqlabelindices, unqlabels, train_losses; appending last index; shifting down\n",
    "\n",
    "LOCINDX = -PlotOnlyTheLastNLabels+1  # From LOCINDX to the end of all lists\n",
    "\n",
    "unqlabelindices = unqlabelindices[LOCINDX:] # Pick desired labels\n",
    "unqlabels = unqlabels[LOCINDX:] # Pick desired labels\n",
    "STARTIND = unqlabelindices[LOCINDX] # Start index [0-] in terms of full train_losses list in file\n",
    "\n",
    "train_losses = train_losses[STARTIND:] # Pick desired part\n",
    "valid_losses = valid_losses[STARTIND:] # Pick desired part\n",
    "\n",
    "if unqlabels[-1] == desc: desc += '-' # So consecutive experiments with same settings have different labels\n",
    "\n",
    "unqlabels.append(desc) # Include the label of new experiment\n",
    "unqlabelindices.append(howmanyinfile) # Point to where the above label is\n",
    "\n",
    "unqlabelindices = [ind-STARTIND for ind in unqlabelindices] # After trimming, all indices should be shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correct the Loss file: ONLY ONCE!\n",
    "# Open the input file for reading\n",
    "with open(\"D-Lossvalues.b\", \"r\") as input_file:\n",
    "    # Read the lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Process the lines from 1 to 87\n",
    "processed_lines = []\n",
    "for line in lines[:87]:\n",
    "    parts = line.strip().split(\", \")\n",
    "    if len(parts) == 3:\n",
    "        new_line = f\"{parts[0]}, {float(parts[1]):020.15f}, {float(parts[1]):020.15f}, {parts[2]}\"\n",
    "        processed_lines.append(new_line)\n",
    "\n",
    "# Process the lines from 88 till the end\n",
    "for line in lines[87:]:\n",
    "    parts = line.strip().split(\", \")\n",
    "    if len(parts) == 4:\n",
    "        new_line = f\"{parts[0]}, {float(parts[1]):020.15f}, {float(parts[3]):020.15f}, {parts[2]}\"\n",
    "        processed_lines.append(new_line)\n",
    "\n",
    "# Write the processed lines to the output file\n",
    "with open(\"D-LossValues.txt\", \"w\") as output_file:\n",
    "    output_file.write(\"\\n\".join(processed_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function definition\n",
    "\n",
    "def valid(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(valid_loader): # 1 batch, 1 channel, 3 spatial dims\n",
    "\n",
    "            images = images.to(device)\n",
    "            masks  = masks .to(device)\n",
    "\n",
    "            # images, masks = transform((images, masks))\n",
    "            # images = images[:,:,200:300,200:300,200:300]\n",
    "            # masks  = masks [:,:,200:300,200:300,200:300]\n",
    "\n",
    "            size3d  = [180, 180, 180]\n",
    "            strtind = [150, 150, 150]\n",
    "\n",
    "            images = images[:,:,strtind[0]:strtind[0]+size3d[0],strtind[1]:strtind[1]+size3d[1],strtind[2]:strtind[2]+size3d[2]]\n",
    "            masks  = masks [:,:,strtind[0]:strtind[0]+size3d[0],strtind[1]:strtind[1]+size3d[1],strtind[2]:strtind[2]+size3d[2]]\n",
    "\n",
    "            outputs = model(images) # outputs.size() = torch.Size([1, 3, 180, 180, 180])\n",
    "\n",
    "            loss = criterion(outputs, torch.squeeze(masks, dim=1))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(valid_loader), images[0,0].cpu(), masks[0,0].cpu(), outputs[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### itkwidgets testing\n",
    "# ba = torch.FloatTensor(2, 2, 5, 5, 5)\n",
    "# ba[:,:,2:4,2:4,2:4]\n",
    "\n",
    "# Plotting the SameCrop()\n",
    "\n",
    "for batch_idx, (images, masks) in enumerate(valid_loader): # 1 batch, 1 channel, 3 spatial dims\n",
    "\n",
    "    # images = images.to(device)\n",
    "    # masks  = masks .to(device)\n",
    "\n",
    "    size3d  = [180, 180, 180]\n",
    "    strtind = [150, 150, 150]\n",
    "\n",
    "    # images, masks = transform((images, masks))\n",
    "    # images = images[:,:,200:300,200:300,200:300]\n",
    "    # masks  = masks [:,:,200:300,200:300,200:300]\n",
    "    images = images[:,:,strtind[0]:strtind[0]+size3d[0],strtind[1]:strtind[1]+size3d[1],strtind[2]:strtind[2]+size3d[2]]\n",
    "    masks  = masks [:,:,strtind[0]:strtind[0]+size3d[0],strtind[1]:strtind[1]+size3d[1],strtind[2]:strtind[2]+size3d[2]]\n",
    "   \n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "# pip install itk itkwidgets\n",
    "# import itk\n",
    "# import itkwidgets\n",
    "\n",
    "# Assuming you have a 3D image named 'image'\n",
    "# image = itk.imread('.\\Data\\SPIROMCS-Case54-Vx3.nii.gz')  # Replace with the path to your actual 3D image file\n",
    "# label = itk.imread('.\\Data\\SPIROMCS-Case54-012Labelmap.nii.gz')  # Replace with the path to your actual 3D image file\n",
    "\n",
    "# Display the 3D image using itkwidgets\n",
    "itkwidgets.view(images[0,0].cpu(), label_image = masks[0,0].cpu()) # , label_image=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itk\n",
    "# import itkwidgets\n",
    "\n",
    "# Display the 3D image using itkwidgets\n",
    "itkwidgets.view(images[0,0].cpu(), label_image = masks[0,0].cpu()) # , label_image=label\n",
    "# itkwidgets.view?\n",
    "# mode= 'x'\n",
    "# label_image_weights=\n",
    "# size_limit_3d= 3x1 numpy int64 array, default: [192, 192, 192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loop ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or no access: 'Data\\SPIROMCS-Case36-Vx3.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Central-AV-Seg-R/cvenvpip/lib/python3.9/site-packages/nibabel/loadsave.py:100\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     stat_result \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(filename)\n\u001b[1;32m    101\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data\\\\SPIROMCS-Case36-Vx3.nii.gz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining loop ...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0-Real\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     train_loss, viewtrnimages, viewtrnmasks, viewtrnoutput \u001b[39m=\u001b[39m train(\n\u001b[1;32m     75\u001b[0m         model, train_loader, criterion, optimizer, device) \u001b[39m########\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1-Quick\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     77\u001b[0m     NNN \u001b[39m=\u001b[39m output_size[\u001b[39m0\u001b[39m] \u001b[39m# 40\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mtrain() \u001b[39m###\u001b[39;00m\n\u001b[1;32m      5\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (images, masks) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      8\u001b[0m     \u001b[39m# print(f\"Batch {batch_idx+1} Started\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     masks  \u001b[39m=\u001b[39m masks \u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Central-AV-Seg-R/cvenvpip/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Central-AV-Seg-R/cvenvpip/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Central-AV-Seg-R/cvenvpip/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Central-AV-Seg-R/cvenvpip/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 146\u001b[0m, in \u001b[0;36mCTImageDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 146\u001b[0m     image \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_paths[index])\u001b[39m.\u001b[39mget_fdata()\n\u001b[1;32m    147\u001b[0m     mask  \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_paths [index])\u001b[39m.\u001b[39mget_fdata()\n\u001b[1;32m    148\u001b[0m     image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(image)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat() \u001b[39m### 1-Channel?!\u001b[39;00m\n",
      "File \u001b[0;32m~/Central-AV-Seg-R/cvenvpip/lib/python3.9/site-packages/nibabel/loadsave.py:102\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     stat_result \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstat(filename)\n\u001b[1;32m    101\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo such file or no access: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m stat_result\u001b[39m.\u001b[39mst_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mraise\u001b[39;00m ImageFileError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmpty file: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: 'Data\\SPIROMCS-Case36-Vx3.nii.gz'"
     ]
    }
   ],
   "source": [
    "# Start the training loop, create losses list, save the model\n",
    "\n",
    "mode = ['0-Real', '1-Quick']\n",
    "mode = mode[0]\n",
    "\n",
    "import itkwidgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (log scale)')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "colormap = plt.cm.get_cmap('rainbow')\n",
    "plt.show()\n",
    "\n",
    "tab = widgets.Tab()\n",
    "viewer = []\n",
    "\n",
    "def update_plot(epoch, train_loss, valid_loss, viewer):\n",
    "    \n",
    "    train_losses.append(train_loss) # Append newly generated value\n",
    "    valid_losses.append(valid_loss) # Append newly generated value\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    xx = range(1, len(train_losses)+2) # Add one dummy value at the end for plot continuity purposes\n",
    "    labinds = unqlabelindices + [len(train_losses)] # Add pointer to the updating final index available for plotting\n",
    "\n",
    "    tt = train_losses + [train_losses[-1]] # Add one dummy value at the end for plot continuity purposes\n",
    "    vv = valid_losses + [valid_losses[-1]] # Add one dummy value at the end for plot continuity purposes\n",
    "\n",
    "    for i, label in enumerate(unqlabels):\n",
    "        POE = int(i != len(unqlabels)-1) #PlotOneExtraForPreviousLabels: Deactivates for the latest label\n",
    "        ax.plot(xx[labinds[i]:labinds[i+1]+POE], tt[labinds[i]:labinds[i+1]+POE],\n",
    "                'o-', label=label, color=colormap(i/len(unqlabels)))\n",
    "        ax.plot(xx[labinds[i]:labinds[i+1]+POE], vv[labinds[i]:labinds[i+1]+POE],\n",
    "                'o-', color=(0.8, 0.8, 0.8))\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    if epoch == 0: ax.legend(fontsize='x-small')\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "\n",
    "\n",
    "    # fig2 = plt.figure()\n",
    "\n",
    "    # Add the viewers to separate tabs\n",
    "    tab.children = [vw for vw in viewer]\n",
    "    for i, _ in enumerate(viewer):\n",
    "        tab.set_title(i, 'Valid-'+str(i//2) if i%2 else 'Train-'+str(i//2))\n",
    "    \n",
    "    # Display the tabbed interface\n",
    "\n",
    "    display(tab)\n",
    "    # print(\"Waiting 2 seconds ...\")\n",
    "    # time.sleep(2)\n",
    "\n",
    "    \n",
    "\n",
    "# Open the file in append mode # GMem=3GB\n",
    "with open('D-LossValues.txt', 'a') as file:\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training loop\n",
    "        print('training loop ...')\n",
    "        if mode == '0-Real':\n",
    "            train_loss, viewtrnimages, viewtrnmasks, viewtrnoutput = train(\n",
    "                model, train_loader, criterion, optimizer, device) ########\n",
    "        elif mode == '1-Quick':\n",
    "            NNN = output_size[0] # 40\n",
    "            viewtrnimages = torch.rand(NNN, NNN, NNN)\n",
    "            viewtrnmasks  = torch.randint(0, high=3, size=(NNN, NNN, NNN))\n",
    "\n",
    "            viewtrnoutput = torch.zeros((3, NNN, NNN, NNN))\n",
    "            viewtrnoutput[1,   NNN//3:,   NNN//3:,   NNN//3:] = 1\n",
    "            viewtrnoutput[2, 2*NNN//3:, 2*NNN//3:, 2*NNN//3:] = 2\n",
    "\n",
    "            train_loss = 0.8 #epoch-1\n",
    "        \n",
    "        # Validation loop\n",
    "        print('validation loop ...')\n",
    "        if mode == '0-Real':\n",
    "            valid_loss, viewvalimages, viewvalmasks, viewvaloutput = valid(\n",
    "                model, valid_loader, criterion, device)\n",
    "        elif mode == '1-Quick':\n",
    "            viewvalimages = torch.rand(NNN, NNN, NNN) + 2\n",
    "            viewvalmasks  = torch.randint(0, high=3, size=(NNN, NNN, NNN))\n",
    "\n",
    "            viewvaloutput = torch.zeros((3, NNN, NNN, NNN))\n",
    "            viewvaloutput[1,   NNN//4:,   NNN//4:,   NNN//4:] = 1\n",
    "            viewvaloutput[2, 2*NNN//4:, 2*NNN//4:, 2*NNN//4:] = 2\n",
    "\n",
    "            valid_loss = 1.0 #epoch-1\n",
    "        \n",
    "        # Convert to class labels (assuming CrossEntropyLoss was used)\n",
    "        viewtrnoutput = torch.argmax(viewtrnoutput, dim=0)\n",
    "        viewvaloutput = torch.argmax(viewvaloutput, dim=0)\n",
    "\n",
    "        # Create itkwidgets viewer for each image\n",
    "        if epoch==0:\n",
    "            # viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnmasks ))\n",
    "            viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnoutput))\n",
    "            # viewer.append(itkwidgets.view(viewvalimages, label_image = viewvalmasks ))\n",
    "            viewer.append(itkwidgets.view(viewvalimages, label_image = viewvaloutput))\n",
    "        if epoch==1:\n",
    "            # viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnmasks ))\n",
    "            viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnoutput))\n",
    "            # viewer.append(itkwidgets.view(viewvalimages, label_image = viewvalmasks ))\n",
    "            viewer.append(itkwidgets.view(viewvalimages, label_image = viewvaloutput))\n",
    "        # if epoch==2:\n",
    "        #     # viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnmasks ))\n",
    "        #     viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnoutput))\n",
    "        #     # viewer.append(itkwidgets.view(viewvalimages, label_image = viewvalmasks ))\n",
    "        #     viewer.append(itkwidgets.view(viewvalimages, label_image = viewvaloutput))\n",
    "        # if epoch==3:\n",
    "        #     # viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnmasks ))\n",
    "        #     viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnoutput))\n",
    "        #     # viewer.append(itkwidgets.view(viewvalimages, label_image = viewvalmasks ))\n",
    "        #     viewer.append(itkwidgets.view(viewvalimages, label_image = viewvaloutput))\n",
    "        # if i==2: viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnmasks ))\n",
    "        # if i==3: viewer.append(itkwidgets.view(viewtrnimages, label_image = viewtrnoutput))\n",
    "\n",
    "        # Update the plot\n",
    "        update_plot(epoch, train_loss, valid_loss, viewer)\n",
    "\n",
    "        # Append loss values to file, use custom label, write each number on a new line\n",
    "        if mode == '0-Real':\n",
    "            file.write(f'{LastIteration+epoch+1:07.0f}, ' + f'{train_loss:020.15f}, ' +\n",
    "                   f'{valid_loss:020.15f}, ' + desc + '\\n')\n",
    "\n",
    "        if mode == '1-Quick':\n",
    "            print(\"Waiting 15 seconds ...\")\n",
    "            time.sleep(15)\n",
    "\n",
    "\n",
    "\n",
    "        #### add time/date too! **\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "# Close the plot after the loop finishes\n",
    "plt.close(fig)\n",
    "\n",
    "# Save the trained model\n",
    "if mode == '0-Real':\n",
    "    torch.save(model.state_dict(), \"./Models/M-Model-\"+f'{ModelToLoad+1:02.0f}'+\".pth\") ################### m2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Save as new model, don't overwrite!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GMem=18.7GB-19.4GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 Tabs working fine without for loop\n",
    "import itkwidgets\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Convert to class labels (assuming CrossEntropyLoss was used)\n",
    "viewvaloutput = torch.argmax(viewvaloutput, dim=0)\n",
    "viewtrnoutput = torch.argmax(viewtrnoutput, dim=0)\n",
    "\n",
    "# Create itkwidgets viewer for each image\n",
    "viewer1 = itkwidgets.view(viewvalimages, label_image = viewvalmasks )\n",
    "viewer2 = itkwidgets.view(viewvalimages, label_image = viewvaloutput)\n",
    "viewer3 = itkwidgets.view(viewtrnimages, label_image = viewtrnmasks )\n",
    "viewer4 = itkwidgets.view(viewtrnimages, label_image = viewtrnoutput)\n",
    "\n",
    "# Create tabs and add the viewers to separate tabs\n",
    "tab = widgets.Tab()\n",
    "tab.children = [viewer1, viewer2, viewer3, viewer4]\n",
    "tab.set_title(0, 'Image 1')\n",
    "tab.set_title(1, 'Image 2')\n",
    "tab.set_title(2, 'Image 3')\n",
    "tab.set_title(3, 'Image 4')\n",
    "\n",
    "# Display the tabbed interface\n",
    "display(tab)\n",
    "\n",
    "\n",
    "# itkwidgets.view(viewvalimages, label_image = viewvalmasks) # , label_image=label\n",
    "\n",
    "# mode= 'y',\n",
    "# label_image_blend=0.5,\n",
    "# vmin: list of floats, default: Minimum of the image pixel buffer\n",
    "# interpolation: bool, default: True\n",
    "# size_limit_3d: 3x1 numpy int64 array, default: [192, 192, 192]\n",
    "# sample_distance: float, default: 0.25\n",
    "#     Sampling distance for volume rendering, normalized from 0.0 to 1.0.\n",
    "\n",
    "# itkwidgets.view(viewvalimages, label_image = viewvaloutput) # , label_image=label\n",
    "# viewvaloutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Toy example to fully understand the view behavior\n",
    "\n",
    "import itkwidgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "NNN = 200\n",
    "viewvalimages = np.random.rand(NNN, NNN, NNN)\n",
    "viewtrnimages = np.random.rand(NNN, NNN, NNN)\n",
    "viewbdsimages = np.random.rand(NNN, NNN, NNN)\n",
    "# viewvalimages = np.zeros((NNN, NNN, NNN))\n",
    "# viewtrnimages = np.zeros((NNN, NNN, NNN))\n",
    "# viewbdsimages = np.zeros((NNN, NNN, NNN))\n",
    "\n",
    "print(\"Into the for loop ...\")\n",
    "for i in range(NNN):\n",
    "    for j in range(NNN):\n",
    "        for k in range(NNN):\n",
    "            viewvalimages[i, j, k] = max(i, j, k)\n",
    "print(\"Out of the for loop ...\")\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "tab = widgets.Tab()\n",
    "viewer = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # Create itkwidgets viewer for each image\n",
    "    if i==0: viewer.append(itkwidgets.view(viewvalimages))\n",
    "    if i==1: viewer.append(itkwidgets.view(viewtrnimages))\n",
    "    if i==2: viewer.append(itkwidgets.view(viewbdsimages))\n",
    "    # label_image = np.random.randint(0, high=3, size=(NNN, NNN, NNN))\n",
    "\n",
    "    # if i==0: viewer.append(itkwidgets.view(viewvalimages, label_image = viewvalmasks ))\n",
    "    # if i==1: viewer.append(itkwidgets.view(viewvalimages, label_image = viewvaloutput))\n",
    "    # viewer3 = itkwidgets.view(viewtrnimages, label_image = viewtrnmasks )\n",
    "    # viewer4 = itkwidgets.view(viewtrnimages, label_image = viewtrnoutput)\n",
    "\n",
    "    # Create tabs and add the viewers to separate tabs\n",
    "\n",
    "    tab.children = [vw for vw in viewer]\n",
    "    # tab.set_title(0, 'Image 1')\n",
    "    # tab.set_title(1, 'Image 2')\n",
    "    # tab.set_title(2, 'Image 3')\n",
    "    # tab.set_title(3, 'Image 4')\n",
    "\n",
    "    # Display the tabbed interface\n",
    "    clear_output(wait=True)\n",
    "    display(tab)\n",
    "    print(\"Waiting 2 seconds ...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mitkwidgets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabel_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabel_image_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabel_image_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabel_image_blend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glasbey'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mselect_roi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgradient_opacity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.22\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mopacity_gaussians\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mslicing_planes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshadow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mblend_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'composite'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint_set_colors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint_set_opacities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint_set_representations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint_set_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgeometries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgeometry_colors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgeometry_opacities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mui_collapsed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrotate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mannotations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'v'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "View the image and/or point sets and/or geometries.\n",
      "\n",
      "Creates and returns an ipywidget to visualize an image, and/or point sets\n",
      "and/or geometries .\n",
      "\n",
      "The image can be 2D or 3D. A label map that corresponds to the image can\n",
      "also be provided. The image and label map must have the same size.\n",
      "\n",
      "The type of the image can be an numpy.array, itk.Image,\n",
      "vtk.vtkImageData, pyvista.UniformGrid, imglyb.ReferenceGuardingRandomAccessibleInterval,\n",
      "or a NumPy array-like, e.g. a Dask array.\n",
      "\n",
      "A point set or a sequence of points sets can be visualized. The type of the\n",
      "point set can be an numpy.array (Nx3 array of point positions).\n",
      "\n",
      "A geometry or a sequence of geometries can be visualized. The type of the\n",
      "geometry can be an itk.Mesh.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "General Interface\n",
      "^^^^^^^^^^^^^^^^^\n",
      "\n",
      "ui_collapsed : bool, default: False\n",
      "    Collapse the native widget user interface.\n",
      "\n",
      "rotate : bool, default: False\n",
      "    Continuously rotate the camera around the scene in volume rendering\n",
      "    mode.\n",
      "\n",
      "annotations : bool, default: True\n",
      "    Display annotations describing orientation and the value of a\n",
      "    mouse-position-based data probe.\n",
      "\n",
      "axes : bool, default: False\n",
      "    Display axes.\n",
      "\n",
      "mode: 'x', 'y', 'z', or 'v', default: 'v'\n",
      "    Only relevant for 3D scenes.\n",
      "    Viewing mode:\n",
      "        'x': x-plane\n",
      "        'y': y-plane\n",
      "        'z': z-plane\n",
      "        'v': volume rendering\n",
      "\n",
      "camera: 3x3 numpy float32 array, or vtk.vtkCamera\n",
      "    Camera parameters:\n",
      "        [[position_x,    position_y,    position_z],\n",
      "         [focal_point_x, focal_point_y, focal_point_z],\n",
      "         [view_up_x,     view_up_y,     view_up_z]]\n",
      "\n",
      "background: (red, green, blue) tuple, components from 0.0 to 1.0\n",
      "    Background color. Default is based on the current Jupyter theme.\n",
      "\n",
      "\n",
      "Images\n",
      "^^^^^^\n",
      "\n",
      "image : array_like, itk.Image, or vtk.vtkImageData\n",
      "    The 2D or 3D image to visualize.\n",
      "\n",
      "label_image : array_like, itk.Image, or vtk.vtkImageData\n",
      "    The 2D or 3D label map to visualize. If an image is also provided, the\n",
      "    label map must have the same size.\n",
      "\n",
      "label_image_names : OrderedDict of (label_value, label_name)\n",
      "    String names associated with the integer label values.\n",
      "\n",
      "label_image_weights : 1D numpy float32 array, default: None\n",
      "    Rendering weights, from 0.0 to 1.0, associated labels in the label map.\n",
      "\n",
      "label_image_blend : float, default: 0.5\n",
      "    Label map blend with intensity image, from 0.0 to 1.0.\n",
      "\n",
      "vmin: list of floats, default: Minimum of the image pixel buffer\n",
      "    Value that maps to the minimum of image colormap. A single value\n",
      "    can be provided or a list for multi-component images.\n",
      "\n",
      "vmax: list of floats, default: Maximum of the image pixel buffer\n",
      "    Value that maps to the minimum of image colormap.  A single value can\n",
      "    be provided or a list for multi-component images.\n",
      "\n",
      "cmap: list of colormaps\n",
      "        default:\n",
      "            - single component: 'viridis', 'grayscale' with a label map,\n",
      "            - two components: 'BkCy', 'BkMa'\n",
      "            - three components: 'BkRd', 'BkGn', 'BkBu'\n",
      "    Colormap for each image component. Some valid values available at\n",
      "    itkwidgets.cm.*\n",
      "    Colormaps can also be Nx3 float NumPy arrays from 0.0 to 1.0 for the\n",
      "    red, green, blue points on the map or a\n",
      "    matplotlib.colors.LinearSegmentedColormap.\n",
      "\n",
      "lut: lookup table, default: 'glasbey'\n",
      "    Lookup table for the label map. Some valid values available at\n",
      "    itkwidgets.lut.*\n",
      "\n",
      "select_roi: bool, default: False\n",
      "    Enable an interactive region of interest widget for the image.\n",
      "\n",
      "slicing_planes: bool, default: False\n",
      "    Enable slicing planes on the volume rendering.\n",
      "\n",
      "x_slice: float, default: None\n",
      "    World-space position of the X slicing plane.\n",
      "\n",
      "y_slice: float, default: None\n",
      "    World-space position of the Y slicing plane.\n",
      "\n",
      "z_slice: float, default: None\n",
      "    World-space position of the Z slicing plane.\n",
      "\n",
      "interpolation: bool, default: True\n",
      "    Linear as opposed to nearest neighbor interpolation for image slices.\n",
      "    Note: Interpolation is not currently supported with label maps.\n",
      "\n",
      "gradient_opacity: float, default: 0.22\n",
      "    Gradient opacity for composite volume rendering, in the range (0.0, 1.0].\n",
      "\n",
      "opacity_gaussians: list of list of dict\n",
      "    Volume rendering opacity transfer function Gaussian parameters. For each\n",
      "    image component, multiple Gaussians can be specified.\n",
      "    Default Gaussian parameters:\n",
      "      {'position': 0.5, 'height': 1, 'width': 0.5, 'xBias': 0.51, 'yBias': 0.4}\n",
      "\n",
      "channels: list of booleans\n",
      "    For multi-component images, the components or channels that are enabled.\n",
      "\n",
      "shadow: bool, default: True\n",
      "    Use shadowing with composite volume rendering.\n",
      "\n",
      "blend_mode: 'composite', 'max', 'min', or 'average', default: 'composite'\n",
      "    Volume rendering blend mode.\n",
      "\n",
      "Point Sets\n",
      "^^^^^^^^^^\n",
      "\n",
      "point_sets: point set, or sequence of point sets\n",
      "    The point sets to visualize.\n",
      "\n",
      "point_set_colors: list of (r, g, b) colors\n",
      "    Colors for the N points. See help(matplotlib.colors) for\n",
      "    specification. Defaults to the Glasbey series of categorical colors.\n",
      "\n",
      "point_set_opacities: array of floats, default: [1.0,]*n\n",
      "    Opacity for the point sets, in the range (0.0, 1.0].\n",
      "\n",
      "point_set_sizes: array of unsigned integers, default: [3,]*n\n",
      "    Sizes for the point sets, in pixel size units.\n",
      "\n",
      "point_set_representations: list of strings, default: ['points',]*n\n",
      "    How to represent the point set. One of 'hidden', 'points', or 'spheres'.\n",
      "\n",
      "Geometries\n",
      "^^^^^^^^^^\n",
      "\n",
      "geometries: geometries, or sequence of geometries\n",
      "    The geometries to visualize.\n",
      "\n",
      "geometry_colors: list of RGB colors\n",
      "    Colors for the N geometries. See help(matplotlib.colors) for\n",
      "    specification. Defaults to the Glasbey series of categorical colors.\n",
      "\n",
      "geometry_opacities: list of floats, default: [1.0,]*n\n",
      "    Opacity for the point sets, in the range (0.0, 1.0].\n",
      "\n",
      "\n",
      "Other Parameters\n",
      "----------------\n",
      "\n",
      "units: string, default: ''\n",
      "    Units to display in the scale bar.\n",
      "\n",
      "actors: vtkActor, vtkAssembly, vtkVolume, default: None\n",
      "    List of standard vtk objects, colors are extracted from their properties\n",
      "\n",
      "size_limit_2d: 2x1 numpy int64 array, default: [1024, 1024]\n",
      "    Size limit for 2D image visualization. If the roi is larger than this\n",
      "    size, it will be downsampled for visualization\n",
      "\n",
      "size_limit_3d: 3x1 numpy int64 array, default: [192, 192, 192]\n",
      "    Size limit for 3D image visualization. If the roi is larger than this\n",
      "    size, it will be downsampled for visualization.\n",
      "\n",
      "sample_distance: float, default: 0.25\n",
      "    Sampling distance for volume rendering, normalized from 0.0 to 1.0.\n",
      "    Lower values result in a higher quality rendering. High values improve\n",
      "    the framerate.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "viewer : ipywidget\n",
      "    Display by placing at the end of a Jupyter cell or calling\n",
      "    IPython.display.display. Query or set properties on the object to change\n",
      "    the visualization or retrieve values created by interacting with the\n",
      "    widget.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\seyhosseini\\desktop\\research\\central-av-seg-l\\.venv3pip\\lib\\site-packages\\itkwidgets\\widget_viewer.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "### itkwidgets.view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000.234353847565849'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Some string tests\n",
    "# A = f'{ABCD:020.15f}'\n",
    "# B = f'{LastIteration+epoch+1:07.0f}, ' + f'{train_loss:015.10f}, ' + f'{valid_loss:015.10f}, ' + desc + '\\n'\n",
    "# A\n",
    "# # {:<15.10f}\n",
    "# # with open('D-ABC.txt', 'a') as file:\n",
    "# #     file.write(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer step #1\n",
    "# Load the trained model\n",
    "# model.load_state_dict(torch.load(\"model-03.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "\n",
    "image = nib.load(\"Data\\SPIROMCS-Case54-Vx3.nii.gz\").get_fdata()\n",
    "mask  = nib.load(\"Data\\SPIROMCS-Case54-012Labelmap.nii.gz\").get_fdata()\n",
    "\n",
    "image = torch.from_numpy(image).unsqueeze(0).float() ### Channels=1 !\n",
    "mask  = torch.from_numpy(mask ).unsqueeze(0).long()  ### Changed!\n",
    "\n",
    "input_tensor = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "mask_tensor  = mask .unsqueeze(0).to(device)  # Add batch dimension ############## to(device) after tramsform!\n",
    "\n",
    "input_tensor, mask_tensor = transform((input_tensor, mask_tensor))\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Post-process the output tensor\n",
    "output_tensor = torch.argmax(output_tensor, dim=1)  # Convert to class labels (assuming CrossEntropyLoss was used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save infer artifacts x3 to disk\n",
    "\n",
    "output_array = output_tensor.squeeze(0).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(output_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-OutputArray.nii.gz\")\n",
    "\n",
    "mask_array = mask_tensor.squeeze(dim=(0,1)).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(mask_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-MaskArray.nii.gz\")\n",
    "\n",
    "input_array = input_tensor.squeeze(dim=(0,1)).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(input_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-CroppedImage.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some bells & whistles for validation loop\n",
    "import time\n",
    "\n",
    "# define an empty list called validation_loss that might be helpful later in the early stopping implementation\n",
    "validation_loss = []\n",
    "\n",
    "start_time = time.time()\n",
    "best_loss = np.float('inf')\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    # Validation\n",
    "    #------------\n",
    "    with torch.no_grad(): \n",
    "      running_loss = 0.0\n",
    "      for i, data in enumerate(validloader, 0):\n",
    "          # get the inputs; data is a list of [inputs, labels]\n",
    "          inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "          # forward \n",
    "          outputs = net(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # print statistics\n",
    "          running_loss += loss.item()\n",
    "      epoch_loss = running_loss / (i+1)\n",
    "\n",
    "      # add epoch_loss at each iteration to the validation_loss list which is later used in the early stopping implementation\n",
    "      validation_loss.append(epoch_loss)\n",
    "\n",
    "      print(\"Epoch: \", epoch, \" validation loss: \", '%.3f' % epoch_loss)\n",
    "      # save the best model based on validation loss\n",
    "      if epoch_loss < best_loss:\n",
    "        # torch.save(net.state_dict(), PATH)\n",
    "        best_loss = epoch_loss\n",
    "\n",
    "      # Early stopping implementation goes below: (TODO)\n",
    "\n",
    "time_elap = (time.time() - start_time) // 60\n",
    "print('Finished Training in %d mins' % time_elap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Investigation\n",
    "\n",
    "x = torch.randn(1024**3, device='cuda')\n",
    "\n",
    "# y = x+1\n",
    "# del y\n",
    "# print(x.dtype)\n",
    "# print(1024**3*32/8/1024**3, 'GB')\n",
    "\n",
    "# print(x.element_size()) # 4 Bytes\n",
    "# print(x.nelement()) # 1024**2\n",
    "# print('OnGPUMemory:', x.element_size()*x.nelement()/1024**3, 'GB')\n",
    "\n",
    "print(torch.cuda.memory_allocated()/1024**3)\n",
    "print(torch.cuda.memory_cached()/1024**3)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.max_memory_allocated()/1024**3\n",
    "# torch.cuda.memory_reserved()\n",
    "# torch.cuda.max_memory_reserved()/1024**3\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_stats()\n",
    "# torch.cuda.memory_snapshot()\n",
    "\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.memory_usage()\n",
    "# torch.cuda.list_gpu_processes() ####################################################\n",
    "# print(torch.cuda.memory_summary()) #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas/Notes:\n",
    "# Start Patching!*\n",
    "# Tensor.detach()?!\n",
    "# Normalization ****\n",
    "# Dropout\n",
    "# Several threads and gpus\n",
    "\n",
    "# nn.CrossEntropyLoss(): label_smoothing=0.0?!!\n",
    "\n",
    "# np.prod(input_tensor.size())/8*32 =\n",
    "# print(input_tensor.storage().nbytes())\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install scipy\n",
    "\n",
    "# python -c \"import torch; print(torch.cuda.is_available())\"\n",
    "\n",
    "# Arch:\n",
    "# pip install itk itkwidgets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

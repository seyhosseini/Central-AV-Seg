{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltoload = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated()/1024**3)\n",
    "print(torch.cuda.memory_cached()/1024**3)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# x = torch.randn(1024**3, device='cuda')\n",
    "# device='cuda:0'\n",
    "# x = x.cuda()\n",
    "# del y\n",
    "# print(x.dtype)\n",
    "# print('OnGPUMemory:', x.element_size()*x.nelement()/1024**3, 'GB')\n",
    "\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.max_memory_allocated()/1024**3\n",
    "# torch.cuda.memory_reserved()\n",
    "# torch.cuda.max_memory_reserved()/1024**3\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_stats()\n",
    "# torch.cuda.memory_snapshot()\n",
    "\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.memory_usage()\n",
    "# torch.cuda.list_gpu_processes() ####################################################\n",
    "# print(torch.cuda.memory_summary()) #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Importing ...\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms # Using TorchIO may help in 3D augmentation *\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define your model architecture here\n",
    "# print(\"Defining Classes ...\")\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels , out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module): #\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffZ = x2.size()[2] - x1.size()[2] # NCXYZ\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1 = nn.functional.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2,\n",
    "                                    diffZ // 2, diffZ - diffZ // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module): ### Add dropout!\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "# Define a custom transform class for applying the same random crop\n",
    "class RandomCrop3D: ###\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "\n",
    "        # Get the input size\n",
    "        input_size = inputs.shape[2:] ###\n",
    "\n",
    "        # Calculate the starting index for the crop\n",
    "        start_indexes = [random.randint(0, input_size[i] - self.output_size[i]) for i in range(3)]\n",
    "\n",
    "        # Perform the crop on both inputs and targets\n",
    "        inputs  = inputs [:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        targets = targets[:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "# Define the output size for random cropping\n",
    "output_size = (128, 128, 128)\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    RandomCrop3D(output_size),              # Custom random crop\n",
    "    # transforms.RandomVerticalFlip(),        # Random vertical flipping\n",
    "    # transforms.RandomHorizontalFlip()        # Random horizontal flipping\n",
    "])\n",
    "\n",
    "# Define your dataset class for loading CT images and masks\n",
    "\n",
    "class CTImageDataset(torch.utils.data.Dataset): ###\n",
    "    def __init__(self, image_paths, mask_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = nib.load(self.image_paths[index]).get_fdata()\n",
    "        mask  = nib.load(self.mask_paths [index]).get_fdata()\n",
    "        image = torch.from_numpy(image).unsqueeze(0).float() ### 1-Channel?!\n",
    "        mask  = torch.from_numpy(mask ).unsqueeze(0).long() ### Changed!\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training function\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device): ###\n",
    "    model.train() ###\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        # print(f\"Batch {batch_idx+1} Started\")\n",
    "\n",
    "        images = images.to(device)\n",
    "        masks  = masks .to(device)\n",
    "\n",
    "        # Apply transforms to the inputs and targets\n",
    "        images, masks = transform((images, masks))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # print(\"Passing through Model ...\")\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        # print(\"CrossEnthropy() ...\")\n",
    "        loss = criterion(outputs, torch.squeeze(masks, dim=1)) #####################################\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # print(\"Backward ...\")\n",
    "        loss.backward()\n",
    "        # print(\"Step ...\")\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your training parameters\n",
    "# print(\"Setting Parameters & Instanciating ...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ####1\n",
    "epochs = 10\n",
    "batch_size = 1 #4 ###\n",
    "learning_rate = 0.0001 #0.001 ###\n",
    "\n",
    "# Create your model instance\n",
    "\n",
    "model = UNet3D(in_channels=1, out_channels=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 40158723\n"
     ]
    }
   ],
   "source": [
    "# Count the number of trainable parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_parameters}\")\n",
    "\n",
    "# Model size\n",
    "print(\"Netwrok size assuming float32bit:\", 40158723*32/8/1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model-\"+f'{modeltoload:02.0f}'+\".pth\")) ########################## m1\n",
    "\n",
    "# Create your dataset and data loader instances\n",
    "\n",
    "image_paths_train = [\"Data\\SPIROMCS-Case36-Vx3.nii.gz\"        , \"Data\\SPIROMCS-Case43-Vx3.nii.gz\"]\n",
    "mask_paths_train  = [\"Data\\SPIROMCS-Case36-012Labelmap.nii.gz\", \"Data\\SPIROMCS-Case43-012Labelmap.nii.gz\"]\n",
    "train_dataset = CTImageDataset(image_paths_train, mask_paths_train) ### Cases 43&36 ### M:1 A:2 V:3 > 012!\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) ### Mask: B=1?C=1?XYZ? #shuffle=True\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "\n",
    "weight = torch.tensor([1,10,10], dtype=torch.float, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) ####2 ignore_index (int, optional) *** #### Using max of all N losses?!!!!\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "losscurve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 24.00 GiB total capacity; 22.03 GiB already allocated; 0 bytes free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart Training ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device) \u001b[39m########\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     losscurve\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[75], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (images, masks) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      8\u001b[0m     \u001b[39m# print(f\"Batch {batch_idx+1} Started\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m     masks  \u001b[39m=\u001b[39m masks \u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Apply transforms to the inputs and targets\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     images, masks \u001b[39m=\u001b[39m transform((images, masks))\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 24.00 GiB total capacity; 22.03 GiB already allocated; 0 bytes free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Start the training loop\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device) ########\n",
    "    losscurve.append(train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "# \"A\"+str(3)  # f'{3:02.0f}'\n",
    "torch.save(model.state_dict(), \"model-\"+f'{modeltoload+1:02.0f}'+\".pth\") ######################## m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6135790646076202,\n",
       " 0.6837256550788879,\n",
       " 0.6177888214588165,\n",
       " 0.6225199997425079,\n",
       " 0.7431220412254333,\n",
       " 0.5967084169387817,\n",
       " 0.7055926322937012,\n",
       " 0.6089262068271637,\n",
       " 0.5874010324478149,\n",
       " 0.619461715221405]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losscurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# model.load_state_dict(torch.load(\"model-03.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "image = nib.load(\"Data\\SPIROMCS-Case36-Vx3.nii.gz\").get_fdata()\n",
    "mask  = nib.load(\"Data\\SPIROMCS-Case36-012Labelmap.nii.gz\").get_fdata()\n",
    "\n",
    "image = torch.from_numpy(image).unsqueeze(0).float() ### Channels=1 !\n",
    "mask  = torch.from_numpy(mask ).unsqueeze(0).long()  ### Changed!\n",
    "\n",
    "input_tensor = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "mask_tensor  = mask .unsqueeze(0).to(device)  # Add batch dimension ############## to(device) after tramsform!\n",
    "\n",
    "input_tensor, mask_tensor = transform((input_tensor, mask_tensor))\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Post-process the output tensor\n",
    "output_tensor = torch.argmax(output_tensor, dim=1)  # Convert to class labels (assuming CrossEntropyLoss was used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = output_tensor.squeeze(0).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(output_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-OutputArray.nii.gz\")\n",
    "\n",
    "mask_array = mask_tensor.squeeze(dim=(0,1)).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(mask_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-MaskArray.nii.gz\")\n",
    "\n",
    "input_array = input_tensor.squeeze(dim=(0,1)).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(input_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-CroppedImage.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Investigation\n",
    "\n",
    "x = torch.randn(1024**3, device='cuda')\n",
    "\n",
    "# y = x+1\n",
    "# del y\n",
    "# print(x.dtype)\n",
    "# print(1024**3*32/8/1024**3, 'GB')\n",
    "\n",
    "# print(x.element_size()) # 4 Bytes\n",
    "# print(x.nelement()) # 1024**2\n",
    "# print('OnGPUMemory:', x.element_size()*x.nelement()/1024**3, 'GB')\n",
    "\n",
    "print(torch.cuda.memory_allocated()/1024**3)\n",
    "print(torch.cuda.memory_cached()/1024**3)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.max_memory_allocated()/1024**3\n",
    "# torch.cuda.memory_reserved()\n",
    "# torch.cuda.max_memory_reserved()/1024**3\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_stats()\n",
    "# torch.cuda.memory_snapshot()\n",
    "\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.memory_usage()\n",
    "# torch.cuda.list_gpu_processes() ####################################################\n",
    "# print(torch.cuda.memory_summary()) #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas/Notes:\n",
    "# Normalization ****\n",
    "# Dropout\n",
    "# Several threads and gpus\n",
    "\n",
    "# nn.CrossEntropyLoss(): label_smoothing=0.0?!!\n",
    "\n",
    "# np.prod(input_tensor.size())/8*32 =\n",
    "# print(input_tensor.storage().nbytes())\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install scipy\n",
    "\n",
    "# python -c \"import torch; print(torch.cuda.is_available())\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

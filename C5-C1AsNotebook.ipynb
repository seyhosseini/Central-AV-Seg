{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelToLoad = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated()/1024**3)\n",
    "print(torch.cuda.memory_cached()/1024**3)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# x = torch.randn(1024**3, device='cuda')\n",
    "# device='cuda:0'\n",
    "# x = x.cuda()\n",
    "# del y\n",
    "# print(x.dtype)\n",
    "# print('OnGPUMemory:', x.element_size()*x.nelement()/1024**3, 'GB')\n",
    "\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.max_memory_allocated()/1024**3\n",
    "# torch.cuda.memory_reserved()\n",
    "# torch.cuda.max_memory_reserved()/1024**3\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_stats()\n",
    "# torch.cuda.memory_snapshot()\n",
    "\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.memory_usage()\n",
    "# torch.cuda.list_gpu_processes() ####################################################\n",
    "# print(torch.cuda.memory_summary()) #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Importing ...\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms # Using TorchIO may help in 3D augmentation *\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define your model architecture here\n",
    "# print(\"Defining Classes ...\")\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels , out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module): #\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffZ = x2.size()[2] - x1.size()[2] # NCXYZ\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1 = nn.functional.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2,\n",
    "                                    diffZ // 2, diffZ - diffZ // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module): ### Add dropout!\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "# Define a custom transform class for applying the same random crop\n",
    "class RandomCrop3D: ###\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "\n",
    "        # Get the input size\n",
    "        input_size = inputs.shape[2:] ###\n",
    "\n",
    "        # Calculate the starting index for the crop\n",
    "        start_indexes = [random.randint(0, input_size[i] - self.output_size[i]) for i in range(3)]\n",
    "\n",
    "        # Perform the crop on both inputs and targets\n",
    "        inputs  = inputs [:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        targets = targets[:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "# Define the output size for random cropping\n",
    "output_size = (128, 128, 128)\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    RandomCrop3D(output_size),              # Custom random crop\n",
    "    # transforms.RandomVerticalFlip(),        # Random vertical flipping\n",
    "    # transforms.RandomHorizontalFlip()        # Random horizontal flipping\n",
    "])\n",
    "\n",
    "# Define your dataset class for loading CT images and masks\n",
    "\n",
    "class CTImageDataset(torch.utils.data.Dataset): ###\n",
    "    def __init__(self, image_paths, mask_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = nib.load(self.image_paths[index]).get_fdata()\n",
    "        mask  = nib.load(self.mask_paths [index]).get_fdata()\n",
    "        image = torch.from_numpy(image).unsqueeze(0).float() ### 1-Channel?!\n",
    "        mask  = torch.from_numpy(mask ).unsqueeze(0).long() ### Changed!\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training function\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device): ###\n",
    "    model.train() ###\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        # print(f\"Batch {batch_idx+1} Started\")\n",
    "\n",
    "        images = images.to(device)\n",
    "        masks  = masks .to(device)\n",
    "\n",
    "        # Apply transforms to the inputs and targets\n",
    "        images, masks = transform((images, masks))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # print(\"Passing through Model ...\")\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        # print(\"CrossEnthropy() ...\")\n",
    "        loss = criterion(outputs, torch.squeeze(masks, dim=1)) #####################################\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # print(\"Backward ...\")\n",
    "        loss.backward()\n",
    "        # print(\"Step ...\")\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your training parameters\n",
    "# print(\"Setting Parameters & Instanciating ...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ####1\n",
    "epochs = 10\n",
    "batch_size = 1 #4 ###\n",
    "learning_rate = 0.0001 #0.001 ###\n",
    "\n",
    "# Create your model instance\n",
    "\n",
    "model = UNet3D(in_channels=1, out_channels=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 40158723\n",
      "Model size assuming float32bit weights: 153.1933708190918 MB\n"
     ]
    }
   ],
   "source": [
    "# Count the number of trainable parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_parameters}\")\n",
    "# Number of trainable parameters: 40158723\n",
    "\n",
    "# Model size\n",
    "print(\"Model size assuming float32bit weights:\", 40158723*32/8/1024**2, \"MB\")\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-------------------+\n",
      "|                  Mod name                 | Parameters Listed |\n",
      "+-------------------------------------------+-------------------+\n",
      "|          inc.double_conv.0.weight         |        1728       |\n",
      "|           inc.double_conv.0.bias          |         64        |\n",
      "|          inc.double_conv.1.weight         |         64        |\n",
      "|           inc.double_conv.1.bias          |         64        |\n",
      "|          inc.double_conv.3.weight         |       110592      |\n",
      "|           inc.double_conv.3.bias          |         64        |\n",
      "|          inc.double_conv.4.weight         |         64        |\n",
      "|           inc.double_conv.4.bias          |         64        |\n",
      "| down1.maxpool_conv.1.double_conv.0.weight |       221184      |\n",
      "|  down1.maxpool_conv.1.double_conv.0.bias  |        128        |\n",
      "| down1.maxpool_conv.1.double_conv.1.weight |        128        |\n",
      "|  down1.maxpool_conv.1.double_conv.1.bias  |        128        |\n",
      "| down1.maxpool_conv.1.double_conv.3.weight |       442368      |\n",
      "|  down1.maxpool_conv.1.double_conv.3.bias  |        128        |\n",
      "| down1.maxpool_conv.1.double_conv.4.weight |        128        |\n",
      "|  down1.maxpool_conv.1.double_conv.4.bias  |        128        |\n",
      "| down2.maxpool_conv.1.double_conv.0.weight |       884736      |\n",
      "|  down2.maxpool_conv.1.double_conv.0.bias  |        256        |\n",
      "| down2.maxpool_conv.1.double_conv.1.weight |        256        |\n",
      "|  down2.maxpool_conv.1.double_conv.1.bias  |        256        |\n",
      "| down2.maxpool_conv.1.double_conv.3.weight |      1769472      |\n",
      "|  down2.maxpool_conv.1.double_conv.3.bias  |        256        |\n",
      "| down2.maxpool_conv.1.double_conv.4.weight |        256        |\n",
      "|  down2.maxpool_conv.1.double_conv.4.bias  |        256        |\n",
      "| down3.maxpool_conv.1.double_conv.0.weight |      3538944      |\n",
      "|  down3.maxpool_conv.1.double_conv.0.bias  |        512        |\n",
      "| down3.maxpool_conv.1.double_conv.1.weight |        512        |\n",
      "|  down3.maxpool_conv.1.double_conv.1.bias  |        512        |\n",
      "| down3.maxpool_conv.1.double_conv.3.weight |      7077888      |\n",
      "|  down3.maxpool_conv.1.double_conv.3.bias  |        512        |\n",
      "| down3.maxpool_conv.1.double_conv.4.weight |        512        |\n",
      "|  down3.maxpool_conv.1.double_conv.4.bias  |        512        |\n",
      "| down4.maxpool_conv.1.double_conv.0.weight |      7077888      |\n",
      "|  down4.maxpool_conv.1.double_conv.0.bias  |        512        |\n",
      "| down4.maxpool_conv.1.double_conv.1.weight |        512        |\n",
      "|  down4.maxpool_conv.1.double_conv.1.bias  |        512        |\n",
      "| down4.maxpool_conv.1.double_conv.3.weight |      7077888      |\n",
      "|  down4.maxpool_conv.1.double_conv.3.bias  |        512        |\n",
      "| down4.maxpool_conv.1.double_conv.4.weight |        512        |\n",
      "|  down4.maxpool_conv.1.double_conv.4.bias  |        512        |\n",
      "|       up1.conv.double_conv.0.weight       |      7077888      |\n",
      "|        up1.conv.double_conv.0.bias        |        256        |\n",
      "|       up1.conv.double_conv.1.weight       |        256        |\n",
      "|        up1.conv.double_conv.1.bias        |        256        |\n",
      "|       up1.conv.double_conv.3.weight       |      1769472      |\n",
      "|        up1.conv.double_conv.3.bias        |        256        |\n",
      "|       up1.conv.double_conv.4.weight       |        256        |\n",
      "|        up1.conv.double_conv.4.bias        |        256        |\n",
      "|       up2.conv.double_conv.0.weight       |      1769472      |\n",
      "|        up2.conv.double_conv.0.bias        |        128        |\n",
      "|       up2.conv.double_conv.1.weight       |        128        |\n",
      "|        up2.conv.double_conv.1.bias        |        128        |\n",
      "|       up2.conv.double_conv.3.weight       |       442368      |\n",
      "|        up2.conv.double_conv.3.bias        |        128        |\n",
      "|       up2.conv.double_conv.4.weight       |        128        |\n",
      "|        up2.conv.double_conv.4.bias        |        128        |\n",
      "|       up3.conv.double_conv.0.weight       |       442368      |\n",
      "|        up3.conv.double_conv.0.bias        |         64        |\n",
      "|       up3.conv.double_conv.1.weight       |         64        |\n",
      "|        up3.conv.double_conv.1.bias        |         64        |\n",
      "|       up3.conv.double_conv.3.weight       |       110592      |\n",
      "|        up3.conv.double_conv.3.bias        |         64        |\n",
      "|       up3.conv.double_conv.4.weight       |         64        |\n",
      "|        up3.conv.double_conv.4.bias        |         64        |\n",
      "|       up4.conv.double_conv.0.weight       |       221184      |\n",
      "|        up4.conv.double_conv.0.bias        |         64        |\n",
      "|       up4.conv.double_conv.1.weight       |         64        |\n",
      "|        up4.conv.double_conv.1.bias        |         64        |\n",
      "|       up4.conv.double_conv.3.weight       |       110592      |\n",
      "|        up4.conv.double_conv.3.bias        |         64        |\n",
      "|       up4.conv.double_conv.4.weight       |         64        |\n",
      "|        up4.conv.double_conv.4.bias        |         64        |\n",
      "|              outc.conv.weight             |        192        |\n",
      "|               outc.conv.bias              |         3         |\n",
      "+-------------------------------------------+-------------------+\n",
      "Sum of trained paramters: 40158723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40158723"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Mod name\", \"Parameters Listed\"])\n",
    "    t_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        t_params+=param\n",
    "    print(table)\n",
    "    print(f\"Sum of trained paramters: {t_params}\")\n",
    "    return t_params\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model-\"+f'{ModelToLoad:02.0f}'+\".pth\")) ########################## m1\n",
    "# GMem+=0.5GB\n",
    "\n",
    "# Create your dataset and data loader instances\n",
    "\n",
    "image_paths_train = [\"Data\\SPIROMCS-Case36-Vx3.nii.gz\"        , \"Data\\SPIROMCS-Case43-Vx3.nii.gz\"]\n",
    "mask_paths_train  = [\"Data\\SPIROMCS-Case36-012Labelmap.nii.gz\", \"Data\\SPIROMCS-Case43-012Labelmap.nii.gz\"]\n",
    "train_dataset = CTImageDataset(image_paths_train, mask_paths_train) ### Cases 43&36 ### M:1 A:2 V:3 > 012!\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) ### Mask: B=1?C=1?XYZ? #shuffle=True\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "\n",
    "weight = torch.tensor([1,10,10], dtype=torch.float, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight) ####2 ignore_index (int, optional) *** #### Using max of all N losses?!!!!\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04761905, 0.47619048, 0.47619048])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get label distribution\n",
    "# Here!\n",
    "# np.array([1,10,10])/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "losscurve = []\n",
    "# GMem=3GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "Epoch 1/10, Train Loss: 0.6318\n",
      "Epoch 2/10, Train Loss: 0.6584\n",
      "Epoch 3/10, Train Loss: 0.5783\n",
      "Epoch 4/10, Train Loss: 0.9926\n",
      "Epoch 5/10, Train Loss: 0.6075\n",
      "Epoch 6/10, Train Loss: 0.8646\n",
      "Epoch 7/10, Train Loss: 0.7436\n",
      "Epoch 8/10, Train Loss: 0.6363\n",
      "Epoch 9/10, Train Loss: 0.5969\n",
      "Epoch 10/10, Train Loss: 0.5911\n"
     ]
    }
   ],
   "source": [
    "# Start the training loop\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device) ########\n",
    "    losscurve.append(train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "# \"A\"+str(3)  # f'{3:02.0f}'\n",
    "torch.save(model.state_dict(), \"model-\"+f'{ModelToLoad+1:02.0f}'+\".pth\") ######################## m2\n",
    "# GMem=18.7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losscurve\n",
    "# GMem=19GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LastIteration: 140\n"
     ]
    }
   ],
   "source": [
    "LastIteration = 130\n",
    "\n",
    "# Open the file in append mode\n",
    "with open('D-Lossvalues.b', 'a') as file:\n",
    "    # Write each number on a new line\n",
    "    for indx, num in enumerate(losscurve):\n",
    "        file.write(f'{LastIteration+indx+1:07.0f}, ' + f'{num:015.13f}, ' + \n",
    "            'UsingModel-03|CEntropyWeightAs[1,10,10]|10Iter' + '\\n')\n",
    "\n",
    "LastIteration = LastIteration+indx+1\n",
    "print('LastIteration: ' + str(LastIteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# model.load_state_dict(torch.load(\"model-03.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "image = nib.load(\"Data\\SPIROMCS-Case36-Vx3.nii.gz\").get_fdata()\n",
    "mask  = nib.load(\"Data\\SPIROMCS-Case36-012Labelmap.nii.gz\").get_fdata()\n",
    "\n",
    "image = torch.from_numpy(image).unsqueeze(0).float() ### Channels=1 !\n",
    "mask  = torch.from_numpy(mask ).unsqueeze(0).long()  ### Changed!\n",
    "\n",
    "input_tensor = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "mask_tensor  = mask .unsqueeze(0).to(device)  # Add batch dimension ############## to(device) after tramsform!\n",
    "\n",
    "input_tensor, mask_tensor = transform((input_tensor, mask_tensor))\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Post-process the output tensor\n",
    "output_tensor = torch.argmax(output_tensor, dim=1)  # Convert to class labels (assuming CrossEntropyLoss was used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = output_tensor.squeeze(0).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(output_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-OutputArray.nii.gz\")\n",
    "\n",
    "mask_array = mask_tensor.squeeze(dim=(0,1)).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(mask_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-MaskArray.nii.gz\")\n",
    "\n",
    "input_array = input_tensor.squeeze(dim=(0,1)).cpu().numpy().astype('int16')\n",
    "img = nib.Nifti1Image(input_array, np.eye(4))\n",
    "nib.save(img, \"./Data/36-CroppedImage.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Investigation\n",
    "\n",
    "x = torch.randn(1024**3, device='cuda')\n",
    "\n",
    "# y = x+1\n",
    "# del y\n",
    "# print(x.dtype)\n",
    "# print(1024**3*32/8/1024**3, 'GB')\n",
    "\n",
    "# print(x.element_size()) # 4 Bytes\n",
    "# print(x.nelement()) # 1024**2\n",
    "# print('OnGPUMemory:', x.element_size()*x.nelement()/1024**3, 'GB')\n",
    "\n",
    "print(torch.cuda.memory_allocated()/1024**3)\n",
    "print(torch.cuda.memory_cached()/1024**3)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# torch.cuda.memory_allocated()\n",
    "# torch.cuda.max_memory_allocated()/1024**3\n",
    "# torch.cuda.memory_reserved()\n",
    "# torch.cuda.max_memory_reserved()/1024**3\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_stats()\n",
    "# torch.cuda.memory_snapshot()\n",
    "\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.memory_usage()\n",
    "# torch.cuda.list_gpu_processes() ####################################################\n",
    "# print(torch.cuda.memory_summary()) #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas/Notes:\n",
    "# Normalization ****\n",
    "# Dropout\n",
    "# Several threads and gpus\n",
    "\n",
    "# nn.CrossEntropyLoss(): label_smoothing=0.0?!!\n",
    "\n",
    "# np.prod(input_tensor.size())/8*32 =\n",
    "# print(input_tensor.storage().nbytes())\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install scipy\n",
    "\n",
    "# python -c \"import torch; print(torch.cuda.is_available())\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

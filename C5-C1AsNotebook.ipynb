{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Importing ...\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms # Using TorchIO may help in 3D augmentation *\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define your model architecture here\n",
    "# print(\"Defining Classes ...\")\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels , out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module): #\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffZ = x2.size()[2] - x1.size()[2] # NCXYZ\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1 = nn.functional.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2,\n",
    "                                    diffZ // 2, diffZ - diffZ // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module): ### Add dropout!\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define a custom transform class for applying the same random crop\n",
    "class RandomCrop3D: ###\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "\n",
    "        # Get the input size\n",
    "        input_size = inputs.shape[2:] ###\n",
    "\n",
    "        # Calculate the starting index for the crop\n",
    "        start_indexes = [random.randint(0, input_size[i] - self.output_size[i]) for i in range(3)]\n",
    "\n",
    "        # Perform the crop on both inputs and targets\n",
    "        inputs  = inputs [:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        targets = targets[:,:, start_indexes[0]:start_indexes[0] + self.output_size[0], \n",
    "                               start_indexes[1]:start_indexes[1] + self.output_size[1],\n",
    "                               start_indexes[2]:start_indexes[2] + self.output_size[2]]\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "# Define the output size for random cropping\n",
    "output_size = (128, 128, 128)\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    RandomCrop3D(output_size),              # Custom random crop\n",
    "    # transforms.RandomVerticalFlip(),        # Random vertical flipping\n",
    "    # transforms.RandomHorizontalFlip()        # Random horizontal flipping\n",
    "])\n",
    "\n",
    "\n",
    "# Define your dataset class for loading CT images and masks\n",
    "\n",
    "class CTImageDataset(torch.utils.data.Dataset): ###\n",
    "    def __init__(self, image_paths, mask_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = nib.load(self.image_paths[index]).get_fdata()\n",
    "        mask  = nib.load(self.mask_paths [index]).get_fdata()\n",
    "        image = torch.from_numpy(image).unsqueeze(0).float() ### 1-Channel?!\n",
    "        mask  = torch.from_numpy(mask ).unsqueeze(0).long() ### Changed!\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training function\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device): ###\n",
    "    model.train() ###\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        # print(f\"Batch {batch_idx+1} Started\")\n",
    "\n",
    "        images = images.to(device)\n",
    "        masks  = masks .to(device)\n",
    "\n",
    "        # Apply transforms to the inputs and targets\n",
    "        images, masks = transform((images, masks))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # print(\"Passing through Model ...\")\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        # print(\"CrossEnthropy() ...\")\n",
    "        loss = criterion(outputs, torch.squeeze(masks, dim=1)) ###\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        # print(\"Backward ...\")\n",
    "        loss.backward()\n",
    "        # print(\"Step ...\")\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your training parameters\n",
    "# print(\"Setting Parameters & Instanciating ...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ####1\n",
    "epochs = 10\n",
    "batch_size = 1 #4 ###\n",
    "learning_rate = 0.0001 #0.001 ###\n",
    "\n",
    "# Create your model instance\n",
    "\n",
    "model = UNet3D(in_channels=1, out_channels=3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create your dataset and data loader instances\n",
    "\n",
    "image_paths_train = [\"Data\\SPIROMCS-Case36-Vx3.nii.gz\", \"Data\\SPIROMCS-Case43-Vx3.nii.gz\"]\n",
    "mask_paths_train  = [\"Data\\SPIROMCS-Case36-012Labelmap.nii.gz\", \"Data\\SPIROMCS-Case43-012Labelmap.nii.gz\"]\n",
    "train_dataset = CTImageDataset(image_paths_train, mask_paths_train) ### Cases 43&36 ### M:1 A:2 V:3 > 012!\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) ### Mask: B=1?C=1?XYZ? #shuffle=True\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() ####2 ignore_index (int, optional) ***\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of trainable parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training loop\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device) ########\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\") ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization *******\n",
    "\n",
    "# Load the trained model\n",
    "# model = UNet3D(in_channels=1, out_channels=3)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "image = nib.load(\"Data\\SPIROMCS-Case36-Vx3.nii.gz\").get_fdata()\n",
    "image = torch.from_numpy(image).unsqueeze(0).float() ### Channels=1 !\n",
    "\n",
    "mask  = nib.load(\"Data\\SPIROMCS-Case36-012Labelmap.nii.gz\").get_fdata()\n",
    "mask  = torch.from_numpy(mask ).unsqueeze(0).long() ### Changed!\n",
    "\n",
    "input_tensor = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "mask_tensor  = mask .unsqueeze(0).to(device)  # Add batch dimension ############## to(device) after tramsform!\n",
    "\n",
    "input_tensor, mask_tensor = transform((input_tensor, mask_tensor))\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Post-process the output tensor\n",
    "output_tensor = torch.argmax(output_tensor, dim=1)  # Convert to class labels (assuming CrossEntropyLoss was used)\n",
    "\n",
    "# Convert the output tensor to numpy array\n",
    "output_array = output_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Perform any desired post-processing or visualization on the output_array\n",
    "# ...\n",
    "\n",
    "# Save or display the results\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itkwidgets import view\n",
    "view(image)\n",
    "# view ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "# Several threads and gpus\n",
    "# nn.CrossEntropyLoss(): label_smoothing=0.0?!!\n",
    "\n",
    "# np.prod(input_tensor.size())/8*32 =\n",
    "# print(input_tensor.storage().nbytes())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
